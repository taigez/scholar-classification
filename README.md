# taige-zhang-scholar-classification

## Overview

This moduel aims to solve scholar classification problems, i.e. given the name of a target scholar, extract key information about the target through web-scarpping, using state-of-the-art sentence classifications and content extraction techniques. 

[XLNet](https://arxiv.org/abs/1906.08237) and [Trafilatra](https://trafilatura.readthedocs.io/en/latest/) were selected and fine-tuned to better address the problem.

## Setup

List the steps needed to install your module's dependencies: 

1. The following code is developed in python version 3.10.1 and pip version 23.0

2. Install required packages 
```
pip install -r requirements.txt 
```
3. Scholar folder is a fully functional python project that provides 2 main functions

4. Classification models are archived at https://huggingface.co/Taige/xlnet-edu (xlnet-awd/xlnet-int/xlnet-pos) for local download if pipeline fails

## Overall breakdown

```
taige-zhang-scholar-classification/
    - requirements.txt
    - data/ 
        - archive/
            -- cs.csv                   # a set of cs scholars data set that can be used for performance testing (only contains 3 category)
        -- awd_data.csv                 # academic awards dataset
        -- edu_data.csv                 # education background dataset
        -- int_data.csv                 # research interest dataset
        -- pos_data.csv                 # career position dataset
    - scholar/
        - src/
            - classification/
                -- classifier.py         # can be launched as python project
                -- utils.py              # helper functions
            
    - tuning/
        -- awards.ipynb                  # jupyter notebook that can be used for future fine-tuning
        -- education.ipynb                  
        -- interests.ipynb                 
        -- position.ipynb                  
```



## Functional Design (Usage)

Key functions in **classifier.py**

* Takes as input a string, representing the name of the target scholar and output a dictionary of sentences for every category that we are interested in
```python
    def scholar_search(name: str):
        ... 
        return { 'awd': [str], 'edu': [str], 'int': [str], 'pos': [str] }
```

* Takes as input a string, representing a website that we are interested in and output a dictionary of sentences for every category that we are interested in
```python
    def url_search(url: str):
        ... 
        return { 'awd': [str], 'edu': [str], 'int': [str], 'pos': [str] }
```

## Demo video
Make sure to include a video showing your module in action and how to use it in this section. Github Pages doesn't support this so I am unable to do this here. However, this can be done in your README.md files of your own repo. Follow instructions [here](https://stackoverflow.com/questions/4279611/how-to-embed-a-video-into-github-readme-md) of the accepted answer 


## Algorithmic Design 

There are 3 main steps that this module focuses on to produce classified sentecnes about the target scholar

### 1. Url Generation
URLs are generated by scraping Google using the target scholar's name as the primary keyword. However, it is crucial to note that certain websites, such as LinkedIn pages, present challenges in terms of compatibility with web scrapers, necessitating a case-by-case approach. The quantity of obtained URLs can be regulated, and in the event that an insufficient number of classified sentences are procured from a predetermined number of links (default = 10), additional URLs will be scraped accordingly.

### 2. Content Extraction
The subsequent step primarily is an application of Trafilatra for content extraction. As indicated in the preceding stage, particular websites may necessitate special attention when it comes to extracting content. Trafilatra can be fine-tuned to exclude diverse HTML elements if deemed necessary.

### 3. Classification
Extracted dataset are parsed into individual sentences and cleaned based on the requirement of XLNet. XLNet requires specifically formatted inputs. For each tokenized input sentence, we need to create:

* input ids: a sequence of integers identifying each input token to its index number in the XLNet tokenizer vocabulary
* segment mask: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence
* attention mask: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens
* labels: a single value of 1 or 0. In our task 1 means “Disaster” and 0 means “Not Disaster”

The final dataset will be a balanced dataset with the above characteristics.





## Issues 

* Low training size for position category.
* Research interest category did not achieve 95% accucary on false positivies.


## Future Work

* Multiclass classification pipeline can be modified and combined into one tensor, taking argmax at the final layer for evaluation. 
* Naive Bayes Algorithm can be introduced on multiple rounds of classification process.
* More classes can be added.

## Change log

Fall 2022: [backend-sentence extractor](https://github.com/Forward-UIUC-2022F/taige-zhang-scholar-wiki)

Spring 2023: multi-class sentence classification


## References 

* [XLNet paper](https://arxiv.org/abs/1906.08237): XLNet: Generalized Autoregressive Pretraining for Language Understanding
* [Trafilatra paper](https://aclanthology.org/2021.acl-demo.15/): Trafilatura: A Web Scraping Library and Command-Line Tool for Text Discovery and Extraction



{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430e1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7290a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm,trange\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6075f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3340fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Prior to pursuing her Ph.D., she worked as an ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>He joined the University of Illinois in 2005 a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>After a two-year stint at Harvard, I came to t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Prof. Allain joined Argonne National Laborator...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Prof. Allain joined the faculty at the Univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1460</th>\n",
       "      <td>0</td>\n",
       "      <td>Although I am interested in how these question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1461</th>\n",
       "      <td>0</td>\n",
       "      <td>At CFI, I am currently looking at ethical and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1462</th>\n",
       "      <td>0</td>\n",
       "      <td>Many commentators have worried that such syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1463</th>\n",
       "      <td>0</td>\n",
       "      <td>My research seeks to analyse and explicate the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1464</th>\n",
       "      <td>0</td>\n",
       "      <td>I am interested in what kinds of 'transparency...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1465 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels                                              texts\n",
       "0          0  Prior to pursuing her Ph.D., she worked as an ...\n",
       "1          0  He joined the University of Illinois in 2005 a...\n",
       "2          0  After a two-year stint at Harvard, I came to t...\n",
       "3          0  Prof. Allain joined Argonne National Laborator...\n",
       "4          0  Prof. Allain joined the faculty at the Univers...\n",
       "...      ...                                                ...\n",
       "1460       0  Although I am interested in how these question...\n",
       "1461       0  At CFI, I am currently looking at ethical and ...\n",
       "1462       0  Many commentators have worried that such syste...\n",
       "1463       0  My research seeks to analyse and explicate the...\n",
       "1464       0  I am interested in what kinds of 'transparency...\n",
       "\n",
       "[1465 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('edu_data.csv',sep=\",\",encoding = \"ISO-8859-1\", engine='python',names=['labels','texts'],header=0)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61aa496e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c978674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1258\n",
       "1     207\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53778a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prior to pursuing her Ph.D., she worked as an environmental engineer specializing in air quality, influencing her focus in engineering design with environmental concerns.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sentence data\n",
    "sentences = df_data.texts.to_list()\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "691feb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Get tag labels data\n",
    "labels = df_data.labels.to_list()\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4137671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a dict for mapping id to tag name\n",
    "#tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "# Recommend to set it by manual define, good for reusing\n",
    "# 0:negative, 1: positive\n",
    "tag2idx={'0': 0,'1': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37eb4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping index to name\n",
    "tag2name={tag2idx[key] : key for key in tag2idx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc03222e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b555613",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = 'xlnet-base-cased-spiece.model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b58a7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Len of the sentence must be the same as the training model\n",
    "# See model's 'max_position_embeddings' = 512\n",
    "max_len  = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8867ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With cased model, set do_lower_case = False\n",
    "tokenizer = XLNetTokenizer(vocab_file=vocabulary,do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9148ed6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.:0\n",
      "sentence: Prior to pursuing her Ph.D., she worked as an environmental engineer specializing in air quality, influencing her focus in engineering design with environmental concerns.\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5893, 22, 9758, 62, 4714, 9, 417, 9, 19, 85, 833, 34, 48, 2405, 4954, 16860, 25, 562, 882, 19, 28131, 62, 1304, 25, 3814, 811, 33, 2405, 2011, 9, 4, 3, 4, 4]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:1\n",
      "sentence: He joined the University of Illinois in 2005 after postdoctoral training under Prof. Wei Chen (NAE Member) at Northwestern University and has been leading the Enterprise Systems Optimization Lab.\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 69, 1274, 18, 315, 20, 3900, 25, 1518, 99, 701, 14335, 801, 168, 16514, 9, 7837, 4717, 17, 10, 3648, 695, 6978, 11, 38, 20235, 315, 21, 51, 72, 895, 18, 11293, 5609, 31427, 9954, 9, 4, 3, 4, 4]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:2\n",
      "sentence: After a two-year stint at Harvard, I came to the University of Illinois at Urbana-Champaign, where I am a Professor with the Industrial and Enterprise Systems Engineering Department.\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 320, 24, 87, 13, 295, 15068, 38, 5461, 19, 35, 301, 22, 18, 315, 20, 3900, 38, 9359, 101, 13, 323, 1714, 1831, 5486, 19, 131, 35, 569, 24, 4147, 33, 18, 7344, 21, 11293, 5609, 6150, 760, 9, 4, 3, 4, 4]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_len  = 64\n",
    "\n",
    "full_input_ids = []\n",
    "full_input_masks = []\n",
    "full_segment_ids = []\n",
    "\n",
    "SEG_ID_A   = 0\n",
    "SEG_ID_B   = 1\n",
    "SEG_ID_CLS = 2\n",
    "SEG_ID_SEP = 3\n",
    "SEG_ID_PAD = 4\n",
    "\n",
    "UNK_ID = tokenizer.encode(\"\")[0]\n",
    "CLS_ID = tokenizer.encode(\"\")[0]\n",
    "SEP_ID = tokenizer.encode(\"\")[0]\n",
    "MASK_ID = tokenizer.encode(\"\")[0]\n",
    "EOD_ID = tokenizer.encode(\"\")[0]\n",
    "\n",
    "for i,sentence in enumerate(sentences):\n",
    "    # Tokenize sentence to token id list\n",
    "    tokens_a = tokenizer.encode(sentence)\n",
    "    \n",
    "    # Trim the len of text\n",
    "    if(len(tokens_a)>max_len-2):\n",
    "        tokens_a = tokens_a[:max_len-2]\n",
    "        \n",
    "        \n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    \n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(SEG_ID_A)\n",
    "        \n",
    "    # Add  token \n",
    "    tokens.append(SEP_ID)\n",
    "    segment_ids.append(SEG_ID_A)\n",
    "    \n",
    "    \n",
    "    # Add  token\n",
    "    tokens.append(CLS_ID)\n",
    "    segment_ids.append(SEG_ID_CLS)\n",
    "    \n",
    "    input_ids = tokens\n",
    "    \n",
    "    # The mask has 0 for real tokens and 1 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length at fornt\n",
    "    if len(input_ids) < max_len:\n",
    "        delta_len = max_len - len(input_ids)\n",
    "        input_ids = [0] * delta_len + input_ids\n",
    "        input_mask = [1] * delta_len + input_mask\n",
    "        segment_ids = [SEG_ID_PAD] * delta_len + segment_ids\n",
    "\n",
    "    assert len(input_ids) == max_len\n",
    "    assert len(input_mask) == max_len\n",
    "    assert len(segment_ids) == max_len\n",
    "    \n",
    "    full_input_ids.append(input_ids)\n",
    "    full_input_masks.append(input_mask)\n",
    "    full_segment_ids.append(segment_ids)\n",
    "    \n",
    "    if 3 > i:\n",
    "        print(\"No.:%d\"%(i))\n",
    "        print(\"sentence: %s\"%(sentence))\n",
    "        print(\"input_ids:%s\"%(input_ids))\n",
    "        print(\"attention_masks:%s\"%(input_mask))\n",
    "        print(\"segment_ids:%s\"%(segment_ids))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb8e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Make label into id\n",
    "tags = [tag2idx[str(lab)] for lab in labels]\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fddb3d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags,tr_masks, val_masks,tr_segs, val_segs = train_test_split(full_input_ids, tags,full_input_masks,full_segment_ids, \n",
    "                                                            random_state=4, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1882f7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1025, 440, 1025, 440)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_inputs),len(val_inputs),len(tr_segs),len(val_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b7c164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "tr_segs = torch.tensor(tr_segs)\n",
    "val_segs = torch.tensor(val_segs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80121bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch num\n",
    "batch_num = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2228073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set token embedding, attention embedding, segment embedding\n",
    "train_data = TensorDataset(tr_inputs, tr_masks,tr_segs, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# Drop last can make batch training better for the last one\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks,val_segs, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11c4f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this document, contain confg(txt) and weight(bin) files\n",
    "# The folder must contain: pytorch_model.bin, config.json\n",
    "model_file_address = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69247edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at models and are newly initialized: ['sequence_summary.summary.bias', 'sequence_summary.summary.weight', 'logits_proj.bias', 'logits_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained(model_file_address,num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f8637d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to GPU,if you are using GPU machine\n",
    "model.to(device);\n",
    "if n_gpu >1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a34a67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33df1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_optimization_steps = int( math.ceil(len(tr_inputs) / batch_num) / 1) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d65c74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af2c8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FULL_FINETUNING:\n",
    "    # Fine tune model all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # Only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7c29f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN loop\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1c9a0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1025\n",
      "  Batch size = 32\n",
      "  Num steps = 660\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|███▊                                                                        | 1/20 [00:13<04:18, 13.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.2605514662573114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  10%|███████▌                                                                    | 2/20 [00:22<03:10, 10.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.11806896660709754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  15%|███████████▍                                                                | 3/20 [00:30<02:43,  9.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08139364294765983\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  20%|███████████████▏                                                            | 4/20 [00:39<02:26,  9.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0764959147782065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  25%|███████████████████                                                         | 5/20 [00:47<02:13,  8.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05852630055960617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  30%|██████████████████████▊                                                     | 6/20 [00:56<02:03,  8.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.05790665420499863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  35%|██████████████████████████▌                                                 | 7/20 [01:04<01:53,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.045307001027595106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  40%|██████████████████████████████▍                                             | 8/20 [01:13<01:43,  8.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04416612389832153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  45%|██████████████████████████████████▏                                         | 9/20 [01:21<01:34,  8.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0349645101432543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  50%|█████████████████████████████████████▌                                     | 10/20 [01:30<01:25,  8.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.0354260455505937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  55%|█████████████████████████████████████████▎                                 | 11/20 [01:38<01:16,  8.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.033711853211571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  60%|█████████████████████████████████████████████                              | 12/20 [01:46<01:06,  8.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.024131426010626456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  65%|████████████████████████████████████████████████▊                          | 13/20 [01:54<00:58,  8.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.038711938265123536\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  70%|████████████████████████████████████████████████████▌                      | 14/20 [02:03<00:50,  8.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.031548459798159456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  75%|████████████████████████████████████████████████████████▎                  | 15/20 [02:11<00:42,  8.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.028864745848522944\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  80%|████████████████████████████████████████████████████████████               | 16/20 [02:20<00:33,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.01898973274137461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  85%|███████████████████████████████████████████████████████████████▊           | 17/20 [02:28<00:25,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02772901282105522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  90%|███████████████████████████████████████████████████████████████████▌       | 18/20 [02:37<00:16,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.033060977663808444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Epoch:  95%|███████████████████████████████████████████████████████████████████████▎   | 19/20 [02:45<00:08,  8.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.024285909738864575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|███████████████████████████████████████████████████████████████████████████| 20/20 [02:54<00:00,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.018717469165039802\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(tr_inputs)))\n",
    "print(\"  Batch size = %d\"%(batch_num))\n",
    "print(\"  Num steps = %d\"%(num_train_optimization_steps))\n",
    "for _ in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_segs,b_labels = batch\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids =b_input_ids,token_type_ids=b_segs, input_mask = b_input_mask,labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "7a813c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalue loop\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9b939a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set acc funtion\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48424b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "69b7ce23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num examples =440\n",
      "  Batch size = 32\n",
      "***** Eval results *****\n",
      "  eval_accuracy = 0.9590909090909091\n",
      "  eval_loss = 0.3312987174124698\n",
      "  loss = 0.018717469165039802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.97      0.98       377\n",
      "           1       0.85      0.87      0.86        63\n",
      "\n",
      "    accuracy                           0.96       440\n",
      "   macro avg       0.91      0.92      0.92       440\n",
      "weighted avg       0.96      0.96      0.96       440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "y_true = []\n",
    "y_predict = []\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num examples ={}\".format(len(val_inputs)))\n",
    "print(\"  Batch size = {}\".format(batch_num))\n",
    "for step, batch in enumerate(valid_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_segs,b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids =b_input_ids,token_type_ids=b_segs, input_mask = b_input_mask,labels=b_labels)\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "    \n",
    "    # Get textclassification predict result\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "#     print(tmp_eval_accuracy)\n",
    "#     print(np.argmax(logits, axis=1))\n",
    "#     print(label_ids)\n",
    "    \n",
    "    # Save predict and real label reuslt for analyze\n",
    "    for predict in np.argmax(logits, axis=1):\n",
    "        y_predict.append(predict)\n",
    "        \n",
    "    for real_result in label_ids.tolist():\n",
    "        y_true.append(real_result)\n",
    "\n",
    "    \n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "   \n",
    "    nb_eval_steps += 1\n",
    "    \n",
    "    \n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / len(val_inputs)\n",
    "loss = tr_loss/nb_tr_steps \n",
    "result = {'eval_loss': eval_loss,\n",
    "                  'eval_accuracy': eval_accuracy,\n",
    "                  'loss': loss}\n",
    "report = classification_report(y_pred=np.array(y_predict),y_true=np.array(y_true))\n",
    "\n",
    "# Save the report into file\n",
    "\n",
    "with open(\"eval_results2.txt\", \"w\") as writer:\n",
    "    print(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        print(\"  %s = %s\"%(key, str(result[key])))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        \n",
    "    print(report)\n",
    "    writer.write(\"\\n\\n\")  \n",
    "    writer.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "742b8f93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('xlnet/xlnet-edu\\\\tokenizer_config.json',\n",
       " 'xlnet/xlnet-edu\\\\special_tokens_map.json',\n",
       " 'xlnet/xlnet-edu\\\\spiece.model',\n",
       " 'xlnet/xlnet-edu\\\\added_tokens.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"xlnet/xlnet-edu\")\n",
    "tokenizer.save_pretrained(\"xlnet/xlnet-edu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70b4534",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "430e1ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7290a64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "from tqdm import tqdm,trange\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6075f238",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (XLNetConfig, XLNetForSequenceClassification, XLNetTokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b3340fd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "      <th>texts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Prof. Allain joined Argonne National Laborator...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Prof. Allain joined the faculty at the Univers...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>During this 5 years he was working closely wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>From 2003-present, Allen has a number of stude...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>In the last 10 years Allen and his students ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1430</th>\n",
       "      <td>0</td>\n",
       "      <td>I am especially interested in reasoning relate...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1431</th>\n",
       "      <td>0</td>\n",
       "      <td>Although I am interested in how these question...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1432</th>\n",
       "      <td>0</td>\n",
       "      <td>At CFI, I am currently looking at ethical and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1433</th>\n",
       "      <td>0</td>\n",
       "      <td>Many commentators have worried that such syste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1434</th>\n",
       "      <td>0</td>\n",
       "      <td>I am interested in what kinds of 'transparency...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1435 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      labels                                              texts\n",
       "0          0  Prof. Allain joined Argonne National Laborator...\n",
       "1          0  Prof. Allain joined the faculty at the Univers...\n",
       "2          0  During this 5 years he was working closely wit...\n",
       "3          0  From 2003-present, Allen has a number of stude...\n",
       "4          0  In the last 10 years Allen and his students ha...\n",
       "...      ...                                                ...\n",
       "1430       0  I am especially interested in reasoning relate...\n",
       "1431       0  Although I am interested in how these question...\n",
       "1432       0  At CFI, I am currently looking at ethical and ...\n",
       "1433       0  Many commentators have worried that such syste...\n",
       "1434       0  I am interested in what kinds of 'transparency...\n",
       "\n",
       "[1435 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data = pd.read_csv('int_data.csv',sep=\",\",encoding = \"ISO-8859-1\", engine='python',names=['labels','texts'],header=0)\n",
    "df_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "61aa496e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5c978674",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1251\n",
       "1     184\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data.labels.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53778a78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Prof. Allain joined Argonne National Laboratory as a staff scientist in 2003 and joined the faculty in the School of Nuclear Engineering at Purdue University in Fall of 2007 with a courtesy appointment with the School of Materials Engineering.'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get sentence data\n",
    "sentences = df_data.texts.to_list()\n",
    "sentences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "691feb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Get tag labels data\n",
    "labels = df_data.labels.to_list()\n",
    "print(labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4137671e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a dict for mapping id to tag name\n",
    "#tag2idx = {t: i for i, t in enumerate(tags_vals)}\n",
    "\n",
    "# Recommend to set it by manual define, good for reusing\n",
    "# 0:negative, 1: positive\n",
    "tag2idx={'0': 0,'1': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37eb4c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping index to name\n",
    "tag2name={tag2idx[key] : key for key in tag2idx.keys()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc03222e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "n_gpu = torch.cuda.device_count()\n",
    "n_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9b555613",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = 'xlnet-base-cased-spiece.model'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b58a7d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Len of the sentence must be the same as the training model\n",
    "# See model's 'max_position_embeddings' = 512\n",
    "max_len  = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba8867ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# With cased model, set do_lower_case = False\n",
    "tokenizer = XLNetTokenizer(vocab_file=vocabulary,do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9148ed6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No.:0\n",
      "sentence: Prof. Allain joined Argonne National Laboratory as a staff scientist in 2003 and joined the faculty in the School of Nuclear Engineering at Purdue University in Fall of 2007 with a courtesy appointment with the School of Materials Engineering.\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16514, 9, 394, 7018, 1274, 17, 27195, 16006, 360, 11842, 34, 24, 891, 8388, 25, 1684, 21, 1274, 18, 4429, 25, 18, 696, 20, 14223, 6150, 38, 24450, 315, 25, 7870, 20, 1327, 33, 24, 14209, 5031, 33, 18, 696, 20, 19093, 6150, 9, 4, 3, 4, 4]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:1\n",
      "sentence: Prof. Allain joined the faculty at the University of Illinois at Urbana-Champaign in the Department of Nuclear, Plasma, and Radiological Engineering in Fall of 2013.\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16514, 9, 394, 7018, 1274, 18, 4429, 38, 18, 315, 20, 3900, 38, 9359, 101, 13, 323, 1714, 1831, 5486, 25, 18, 760, 20, 14223, 19, 8104, 23, 661, 19, 21, 3402, 10838, 6150, 25, 7870, 20, 2521, 9, 4, 3, 4, 4]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n",
      "No.:2\n",
      "sentence: During this 5 years he was working closely with clinical audiologists and speech and hearing scientists, and with several hearing aid manufactures (Starkey, Phonak, Etymotic), who subsequently funded Allen's work.\n",
      "input_ids:[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 940, 52, 306, 123, 43, 30, 481, 3126, 33, 4494, 17, 21735, 11464, 21, 2077, 21, 2243, 3582, 19, 21, 33, 294, 2243, 1443, 8567, 23, 17, 10, 4293, 4264, 19, 21950, 14752, 19, 16695, 11913, 10707, 11, 19, 61, 4742, 7327, 4671, 26, 23, 154, 9, 4, 3, 4, 4]\n",
      "attention_masks:[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "segment_ids:[4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "max_len  = 64\n",
    "\n",
    "full_input_ids = []\n",
    "full_input_masks = []\n",
    "full_segment_ids = []\n",
    "\n",
    "SEG_ID_A   = 0\n",
    "SEG_ID_B   = 1\n",
    "SEG_ID_CLS = 2\n",
    "SEG_ID_SEP = 3\n",
    "SEG_ID_PAD = 4\n",
    "\n",
    "UNK_ID = tokenizer.encode(\"\")[0]\n",
    "CLS_ID = tokenizer.encode(\"\")[0]\n",
    "SEP_ID = tokenizer.encode(\"\")[0]\n",
    "MASK_ID = tokenizer.encode(\"\")[0]\n",
    "EOD_ID = tokenizer.encode(\"\")[0]\n",
    "\n",
    "for i,sentence in enumerate(sentences):\n",
    "    # Tokenize sentence to token id list\n",
    "    tokens_a = tokenizer.encode(sentence)\n",
    "    \n",
    "    # Trim the len of text\n",
    "    if(len(tokens_a)>max_len-2):\n",
    "        tokens_a = tokens_a[:max_len-2]\n",
    "        \n",
    "        \n",
    "    tokens = []\n",
    "    segment_ids = []\n",
    "    \n",
    "    for token in tokens_a:\n",
    "        tokens.append(token)\n",
    "        segment_ids.append(SEG_ID_A)\n",
    "        \n",
    "    # Add  token \n",
    "    tokens.append(SEP_ID)\n",
    "    segment_ids.append(SEG_ID_A)\n",
    "    \n",
    "    \n",
    "    # Add  token\n",
    "    tokens.append(CLS_ID)\n",
    "    segment_ids.append(SEG_ID_CLS)\n",
    "    \n",
    "    input_ids = tokens\n",
    "    \n",
    "    # The mask has 0 for real tokens and 1 for padding tokens. Only real\n",
    "    # tokens are attended to.\n",
    "    input_mask = [0] * len(input_ids)\n",
    "\n",
    "    # Zero-pad up to the sequence length at fornt\n",
    "    if len(input_ids) < max_len:\n",
    "        delta_len = max_len - len(input_ids)\n",
    "        input_ids = [0] * delta_len + input_ids\n",
    "        input_mask = [1] * delta_len + input_mask\n",
    "        segment_ids = [SEG_ID_PAD] * delta_len + segment_ids\n",
    "\n",
    "    assert len(input_ids) == max_len\n",
    "    assert len(input_mask) == max_len\n",
    "    assert len(segment_ids) == max_len\n",
    "    \n",
    "    full_input_ids.append(input_ids)\n",
    "    full_input_masks.append(input_mask)\n",
    "    full_segment_ids.append(segment_ids)\n",
    "    \n",
    "    if 3 > i:\n",
    "        print(\"No.:%d\"%(i))\n",
    "        print(\"sentence: %s\"%(sentence))\n",
    "        print(\"input_ids:%s\"%(input_ids))\n",
    "        print(\"attention_masks:%s\"%(input_mask))\n",
    "        print(\"segment_ids:%s\"%(segment_ids))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb8e88c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "# Make label into id\n",
    "tags = [tag2idx[str(lab)] for lab in labels]\n",
    "print(tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fddb3d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs, val_inputs, tr_tags, val_tags,tr_masks, val_masks,tr_segs, val_segs = train_test_split(full_input_ids, tags,full_input_masks,full_segment_ids, \n",
    "                                                            random_state=4, test_size=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1882f7b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1004, 431, 1004, 431)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tr_inputs),len(val_inputs),len(tr_segs),len(val_segs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3b7c164b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_inputs = torch.tensor(tr_inputs)\n",
    "val_inputs = torch.tensor(val_inputs)\n",
    "tr_tags = torch.tensor(tr_tags)\n",
    "val_tags = torch.tensor(val_tags)\n",
    "tr_masks = torch.tensor(tr_masks)\n",
    "val_masks = torch.tensor(val_masks)\n",
    "tr_segs = torch.tensor(tr_segs)\n",
    "val_segs = torch.tensor(val_segs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80121bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch num\n",
    "batch_num = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e2228073",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set token embedding, attention embedding, segment embedding\n",
    "train_data = TensorDataset(tr_inputs, tr_masks,tr_segs, tr_tags)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "# Drop last can make batch training better for the last one\n",
    "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=True)\n",
    "\n",
    "valid_data = TensorDataset(val_inputs, val_masks,val_segs, val_tags)\n",
    "valid_sampler = SequentialSampler(valid_data)\n",
    "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "11c4f33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this document, contain confg(txt) and weight(bin) files\n",
    "# The folder must contain: pytorch_model.bin, config.json\n",
    "model_file_address = 'models'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69247edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
      "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at models and are newly initialized: ['sequence_summary.summary.weight', 'sequence_summary.summary.bias', 'logits_proj.weight', 'logits_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = XLNetForSequenceClassification.from_pretrained(model_file_address,num_labels=len(tag2idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f8637d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set model to GPU,if you are using GPU machine\n",
    "model.to(device);\n",
    "if n_gpu >1:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a34a67fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "max_grad_norm = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "33df1954",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_optimization_steps = int( math.ceil(len(tr_inputs) / batch_num) / 1) * epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d65c74c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "af2c8545",
   "metadata": {},
   "outputs": [],
   "source": [
    "if FULL_FINETUNING:\n",
    "    # Fine tune model all layer parameters\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = ['bias', 'gamma', 'beta']\n",
    "    optimizer_grouped_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.01},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "         'weight_decay_rate': 0.0}\n",
    "    ]\n",
    "else:\n",
    "    # Only fine tune classifier parameters\n",
    "    param_optimizer = list(model.classifier.named_parameters()) \n",
    "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7c29f31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN loop\n",
    "model.train();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e1c9a0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1004\n",
      "  Batch size = 32\n",
      "  Num steps = 640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|███▊                                                                        | 1/20 [00:08<02:35,  8.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.3345604293769406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  10%|███████▌                                                                    | 2/20 [00:14<02:09,  7.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.20075638351901884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  15%|███████████▍                                                                | 3/20 [00:20<01:55,  6.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.14506366422339792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  20%|███████████████▏                                                            | 4/20 [00:27<01:44,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.12473487616666863\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  25%|███████████████████                                                         | 5/20 [00:34<01:40,  6.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.10569265157532608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  30%|██████████████████████▊                                                     | 6/20 [00:41<01:34,  6.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.08795369270768377\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  35%|██████████████████████████▌                                                 | 7/20 [00:47<01:27,  6.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.09026772210059027\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  40%|██████████████████████████████▍                                             | 8/20 [00:54<01:22,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.07497744024130365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  45%|██████████████████████████████████▏                                         | 9/20 [01:01<01:15,  6.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.048960871235918135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  50%|█████████████████████████████████████▌                                     | 10/20 [01:08<01:09,  6.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.043751173448042885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  55%|█████████████████████████████████████████▎                                 | 11/20 [01:15<01:01,  6.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.04206807208397696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  60%|█████████████████████████████████████████████                              | 12/20 [01:22<00:54,  6.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.03450527352575709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  65%|████████████████████████████████████████████████▊                          | 13/20 [01:28<00:46,  6.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02994080425349004\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  70%|████████████████████████████████████████████████████▌                      | 14/20 [01:35<00:40,  6.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.050098093958137815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  75%|████████████████████████████████████████████████████████▎                  | 15/20 [01:41<00:32,  6.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.027162483550660733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  80%|████████████████████████████████████████████████████████████               | 16/20 [01:47<00:26,  6.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.030392862973742246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  85%|███████████████████████████████████████████████████████████████▊           | 17/20 [01:54<00:19,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.030029947966883606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  90%|███████████████████████████████████████████████████████████████████▌       | 18/20 [02:01<00:13,  6.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02511460006456717\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Epoch:  95%|███████████████████████████████████████████████████████████████████████▎   | 19/20 [02:07<00:06,  6.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.025745229859650512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|███████████████████████████████████████████████████████████████████████████| 20/20 [02:14<00:00,  6.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.02187146436253604\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Running training *****\")\n",
    "print(\"  Num examples = %d\"%(len(tr_inputs)))\n",
    "print(\"  Batch size = %d\"%(batch_num))\n",
    "print(\"  Num steps = %d\"%(num_train_optimization_steps))\n",
    "for _ in trange(epochs,desc=\"Epoch\"):\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        # add batch to gpu\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_segs,b_labels = batch\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids =b_input_ids,token_type_ids=b_segs, input_mask = b_input_mask,labels=b_labels)\n",
    "        loss, logits = outputs[:2]\n",
    "        if n_gpu>1:\n",
    "            # When multi gpu, average it\n",
    "            loss = loss.mean()\n",
    "        \n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        \n",
    "        # track train loss\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "        \n",
    "        # gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n",
    "        \n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    # print train loss per epoch\n",
    "    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7a813c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evalue loop\n",
    "model.eval();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9b939a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set acc funtion\n",
    "def accuracy(out, labels):\n",
    "    outputs = np.argmax(out, axis=1)\n",
    "    return np.sum(outputs == labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48424b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69b7ce23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Running evaluation *****\n",
      "  Num examples =431\n",
      "  Batch size = 32\n",
      "***** Eval results *****\n",
      "  eval_accuracy = 0.9280742459396751\n",
      "  eval_loss = 0.5692870940048513\n",
      "  loss = 0.02187146436253604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.95      0.96       374\n",
      "           1       0.70      0.81      0.75        57\n",
      "\n",
      "    accuracy                           0.93       431\n",
      "   macro avg       0.83      0.88      0.85       431\n",
      "weighted avg       0.93      0.93      0.93       431\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eval_loss, eval_accuracy = 0, 0\n",
    "nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "y_true = []\n",
    "y_predict = []\n",
    "print(\"***** Running evaluation *****\")\n",
    "print(\"  Num examples ={}\".format(len(val_inputs)))\n",
    "print(\"  Batch size = {}\".format(batch_num))\n",
    "for step, batch in enumerate(valid_dataloader):\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_segs,b_labels = batch\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids =b_input_ids,token_type_ids=b_segs, input_mask = b_input_mask,labels=b_labels)\n",
    "        tmp_eval_loss, logits = outputs[:2]\n",
    "    \n",
    "    # Get textclassification predict result\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_eval_accuracy = accuracy(logits, label_ids)\n",
    "#     print(tmp_eval_accuracy)\n",
    "#     print(np.argmax(logits, axis=1))\n",
    "#     print(label_ids)\n",
    "    \n",
    "    # Save predict and real label reuslt for analyze\n",
    "    for predict in np.argmax(logits, axis=1):\n",
    "        y_predict.append(predict)\n",
    "        \n",
    "    for real_result in label_ids.tolist():\n",
    "        y_true.append(real_result)\n",
    "\n",
    "    \n",
    "    eval_loss += tmp_eval_loss.mean().item()\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "   \n",
    "    nb_eval_steps += 1\n",
    "    \n",
    "    \n",
    "eval_loss = eval_loss / nb_eval_steps\n",
    "eval_accuracy = eval_accuracy / len(val_inputs)\n",
    "loss = tr_loss/nb_tr_steps \n",
    "result = {'eval_loss': eval_loss,\n",
    "                  'eval_accuracy': eval_accuracy,\n",
    "                  'loss': loss}\n",
    "report = classification_report(y_pred=np.array(y_predict),y_true=np.array(y_true))\n",
    "\n",
    "# Save the report into file\n",
    "\n",
    "with open(\"eval_results3.txt\", \"w\") as writer:\n",
    "    print(\"***** Eval results *****\")\n",
    "    for key in sorted(result.keys()):\n",
    "        print(\"  %s = %s\"%(key, str(result[key])))\n",
    "        writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "        \n",
    "    print(report)\n",
    "    writer.write(\"\\n\\n\")  \n",
    "    writer.write(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bc0fd24b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During this 5 years he was working closely with clinical audiologists and speech and hearing scientists, and with several hearing aid manufactures (Starkey, Phonak, Etymotic), who subsequently funded Allen's work.\n",
      "---------------------\n",
      "Dr. Marie Agathe Charpagne was born and studied in France, where she was a first generation college student.\n",
      "---------------------\n",
      "After this she joined the Energy Frontier Research Center for Inverse Design as a postdoctoral fellow developing p-type transparent conducting oxides and synthesizing missing materials.\n",
      "---------------------\n",
      "In another path, we develop biomaterials that can home and manipulate immune cells in vivo, and apply them to the development of cancer vaccines, cell therapies, and medical devices.\n",
      "---------------------\n",
      "Students can get involved in projects in various stages, through a variety of ways.\n",
      "---------------------\n",
      "Little is known, however, about the interpersonal mechanisms through which children's attachment and friend relationships are related.\n",
      "---------------------\n",
      "He is particularly interested in using data-driven techniques to tackle problems where large quantities of unlabeled visual data are readily available.\n",
      "---------------------\n",
      "My research interests are at the intersection of machine learning and data systems and my students are working on a wide range of projects including: real-time model serving; machine learning life-cycle management; accelerated deep learning for computer vision; new cryptographic primitives for federated learning; frameworks for deep reinforcement learning and parameter tuning; model based cloud resource management; software platforms for autonomous vehicles research; computational efficient representations for asynchronous time series; smf frameworks for graph query processing.\n",
      "---------------------\n",
      "My interests have long included online learning environments for programming instruction.\n",
      "---------------------\n",
      "He has recently begun to work on aspects of responsible AI, from differential privacy to questions of bias in automatic decision making.\n",
      "---------------------\n",
      "She also worked on medical imaging, and developed with her students a digital anatomy atlas coupled with elastic matching algorithms that made it possible to automatically identify anatomic structures of the brain, first in X-ray tomography, later with MRI and positron image tomography.\n",
      "---------------------\n",
      "In 2011, he returned to academia and received a master's in Computer Science and another master's degree in Engineering and Technology Management from Portland State University.\n",
      "---------------------\n",
      "He then received a doctoral degree in Computer Science from Portland State in 2016.\n",
      "---------------------\n",
      "Haitham Hassanieh is an assistant professor in the Electrical and Computer Engineering and Computer Science departments at the University of Illinois at Urbana Champaign.\n",
      "---------------------\n",
      "He received a Silver Medal from the 32nd International Mathematical Olympiad in 1991, University Medal from the University of Canberra in 1997, Doctorate Award from the EPFL in 2001, CAREER Award from the National Science Foundation in 2003, Xerox Award for Faculty Research from UIUC in 2007, and Young Author Best Paper Award from IEEE in 2008.\n",
      "---------------------\n",
      "I focused on the interaction between drivers and autonomous vehicles.\n",
      "---------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(y_true)):\n",
    "    if y_true[i] == 0 and y_predict[i] == 1:\n",
    "        print(df_data.iloc[i]['texts'])\n",
    "        print(\"---------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c9cf1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
